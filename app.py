# ================================================================
#  Akbank GenAI Bootcamp - Kurumsal SÃ¼rdÃ¼rÃ¼lebilirlik AsistanÄ±
#  ------------------------------------------------------------
#  Bu dosya, RAG (Retrieval-Augmented Generation) mimarisine
#  sahip bir Streamlit uygulamasÄ±dÄ±r. Belgelerdeki sÃ¼rdÃ¼rÃ¼lebilirlik
#  iÃ§eriklerini analiz edip kullanÄ±cÄ± sorularÄ±na baÄŸlam temelli
#  yanÄ±tlar Ã¼retir.
# ================================================================

# --- Gerekli kÃ¼tÃ¼phanelerin import edilmesi ---
import os
import glob
import streamlit as st

# LangChain RAG bileÅŸenleri
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.retrievers.multi_query import MultiQueryRetriever
# PDF yÃ¼kleme ve metin bÃ¶lme
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# FAISS veritabanÄ± dizini (Streamlit Cloud'da /tmp klasÃ¶rÃ¼ yazÄ±labilir olduÄŸu iÃ§in oraya alÄ±nabilir)
FAISS_INDEX_PATH = os.path.join(os.getenv("TMPDIR", "/tmp"), "faiss_index")

# ================================================================
#                  RAG SÄ°STEMÄ°NÄ°N YÃœKLENMESÄ°
# ================================================================

@st.cache_resource
def load_and_build_db():
    """
    TÃ¼m modelleri ve FAISS veritabanÄ±nÄ± yÃ¼kler veya oluÅŸturur.
    Bu iÅŸlem yalnÄ±zca ilk Ã§alÄ±ÅŸtÄ±rmada yapÄ±lÄ±r, Streamlit tarafÄ±ndan cache'lenir.
    """

    print("VeritabanÄ± kontrol ediliyor...")

    # --- 1. EMBEDDING MODELÄ° ---
    # TÃ¼rkÃ§e dahil Ã§ok dilli gÃ¼Ã§lÃ¼ model: paraphrase-multilingual-mpnet-base-v2
    model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    embeddings = HuggingFaceEmbeddings(model_name=model_name)

    # --- 2. FAISS VERÄ°TABANI KONTROLÃœ ---
    if os.path.exists(FAISS_INDEX_PATH):
        print("Mevcut FAISS veritabanÄ± bulundu, yÃ¼kleniyor...")
        db = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
    else:
        print("VeritabanÄ± bulunamadÄ±. PDF'lerden yeni veritabanÄ± oluÅŸturuluyor...")

        # data klasÃ¶rÃ¼ndeki tÃ¼m PDFâ€™leri bul
        pdf_files = glob.glob("data/*.pdf")
        if not pdf_files:
            st.error("HATA: 'data' klasÃ¶rÃ¼nde PDF dosyasÄ± bulunamadÄ±.")
            st.stop()

        all_documents = []
        for pdf_path in pdf_files:
            loader = PyPDFLoader(pdf_path)
            documents = loader.load()
            for doc in documents:
                doc.metadata["source"] = os.path.basename(pdf_path)
            all_documents.extend(documents)

        # --- Metinleri parÃ§alara ayÄ±rma ---
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
        docs = text_splitter.split_documents(all_documents)

        # --- FAISS veritabanÄ± oluÅŸturma ---
        db = FAISS.from_documents(docs, embeddings)
        db.save_local(FAISS_INDEX_PATH)
        print("Yeni FAISS veritabanÄ± oluÅŸturuldu ve kaydedildi.")

    # --- 3. DÄ°L MODELÄ° (LLM) ---
    # Gemini Pro kullanÄ±lÄ±yor; dÃ¼ÅŸÃ¼k temperature deÄŸeri = daha tutarlÄ± yanÄ±tlar
    llm = ChatGoogleGenerativeAI(
        model="gemini-pro-latest",
        temperature=0.1,
        convert_system_message_to_human=True
    )

    # --- 4. MULTIQUERY RETRIEVER ---
    # KullanÄ±cÄ± sorgusunu LLM aracÄ±lÄ±ÄŸÄ±yla farklÄ± aÃ§Ä±lardan yeniden ifade ederek
    # daha kapsamlÄ± bilgi getirimi saÄŸlar.
    base_retriever = db.as_retriever(search_kwargs={'k': 7})
    retriever = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=llm)

    print("Modeller ve retriever baÅŸarÄ±yla hazÄ±rlandÄ±.")
    return retriever, llm


# ================================================================
#               RAG ZÄ°NCÄ°RÄ°NÄ°N OLUÅTURULMASI
# ================================================================
def create_rag_chain(retriever, llm):
    """
    RAG zincirini oluÅŸturur:
      - KullanÄ±cÄ± sorusu alÄ±r
      - Ä°lgili baÄŸlamÄ± retriever'dan Ã§eker
      - LLM'e yÃ¶nlendirir ve sonuÃ§ dÃ¶ner
    """

    # --- 5. PROMPT ÅABLONU ---
    # TÃ¼rkÃ§e aÃ§Ä±klamalÄ±, baÄŸlama sadÄ±k kalan bir prompt.
    template = """
    ### TALÄ°MAT:
    Sana verilen `BAÄLAM` bÃ¶lÃ¼mÃ¼ndeki bilgileri kullanarak `SORU` bÃ¶lÃ¼mÃ¼ndeki soruyu yanÄ±tla.
    Cevap dÄ±ÅŸarÄ±dan bilgi iÃ§ermesin. Net, anlaÅŸÄ±lÄ±r ve sohbet tarzÄ±nda cevap ver.
    EÄŸer baÄŸlamda yanÄ±t yoksa, 'Bu konuda saÄŸlanan dokÃ¼manlarda bilgi bulamadÄ±m.' de.
    CevaplarÄ±nÄ± TÃ¼rkÃ§e ver.

    ### BAÄLAM:
    {context}

    ### SORU:
    {question}

    ### CEVAP:
    """
    prompt = PromptTemplate(template=template, input_variables=["context", "question"])

    # --- 6. LANGCHAIN EXPRESSION LANGUAGE ZÄ°NCÄ°RÄ° ---
    # RAG akÄ±ÅŸÄ±nÄ± tanÄ±mlar.
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain


# ================================================================
#                    STREAMLIT ARAYÃœZÃœ
# ================================================================

st.set_page_config(
    page_title="Kurumsal SÃ¼rdÃ¼rÃ¼lebilirlik AsistanÄ±",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Kenar Ã‡ubuÄŸu ---
with st.sidebar:
    st.image("assets/surdurulebilirlik_venn.png", use_container_width=True)
    st.title("ğŸŒ± Kurumsal SÃ¼rdÃ¼rÃ¼lebilirlik AsistanÄ±")
    st.markdown("""
    Bu asistan, RAG mimarisi kullanarak aÅŸaÄŸÄ±daki belgelerdeki bilgilere gÃ¶re sorularÄ±nÄ±zÄ± yanÄ±tlar:
    - **Erdem & Erdem - Ã‡SY Terimler SÃ¶zlÃ¼ÄŸÃ¼**
    - **Borsa Ä°stanbul - SÃ¼rdÃ¼rÃ¼lebilirlik Rehberi**
    """)
    st.markdown("---")

    if st.button("Sohbeti Temizle", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Akbank GenAI Bootcamp Projesi")

# --- Ã–rnek Sorular ---
st.markdown("""
**Ã–rnek Sorular:**
- SÃ¼rdÃ¼rÃ¼lebilir uygulamalarÄ±n artÄ±rÄ±lmasÄ± ÅŸirkete hangi katkÄ±larÄ± saÄŸlar?
- SÄ±nÄ±rda karbon dÃ¼zenlemesi nedir?
- Paris AnlaÅŸmasÄ± nedir?
- Kurumsal YÃ¶netim nedir?
- Karbon tutma nedir?
""")
st.markdown("---")

# --- API ANAHTARI KONTROLÃœ ---
# Streamlit Cloud'da secrets.toml iÃ§inde, lokal Ã§alÄ±ÅŸmada ise os.environ'dan alÄ±nÄ±r.
api_key = st.secrets.get("GOOGLE_API_KEY", os.getenv("GOOGLE_API_KEY"))
if not api_key:
    st.error("HATA: GOOGLE_API_KEY bulunamadÄ±. LÃ¼tfen secrets veya ortam deÄŸiÅŸkenine ekleyin.")
    st.stop()
os.environ["GOOGLE_API_KEY"] = api_key

# --- RAG Sisteminin BaÅŸlatÄ±lmasÄ± ---
try:
    retriever, llm = load_and_build_db()
    rag_chain = create_rag_chain(retriever, llm)
except Exception as e:
    st.error(f"BaÅŸlatma sÄ±rasÄ±nda hata oluÅŸtu: {e}")
    st.stop()

# --- Sohbet GeÃ§miÅŸi YÃ¶netimi ---
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Merhaba! SÃ¼rdÃ¼rÃ¼lebilirlik veya Ã‡SY konularÄ±nda nasÄ±l yardÄ±mcÄ± olabilirim?"}
    ]

# GeÃ§miÅŸ mesajlarÄ± yazdÄ±r
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Yeni KullanÄ±cÄ± Girdisi ---
if prompt := st.chat_input("SÃ¼rdÃ¼rÃ¼lebilirlik stratejisi, raporlama veya bir terim hakkÄ±nda soru sorun..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Belgeler analiz ediliyor ve yanÄ±t hazÄ±rlanÄ±yor..."):
            # Belge kaynaklarÄ±nÄ± getir (MultiQueryRetriever kullandÄ±ÄŸÄ±mÄ±z iÃ§in base retriever Ã¼zerinden)
            try:
                retrieved_docs = retriever.retriever.get_relevant_documents(prompt)
            except:
                retrieved_docs = []

            # CevabÄ± stream halinde yazdÄ±r
            response_stream = rag_chain.stream(prompt)
            full_response = st.write_stream(response_stream)

            # KaynaklarÄ± gÃ¶ster
            with st.expander("YanÄ±tÄ±n KaynaklarÄ±nÄ± GÃ¶r"):
                if retrieved_docs:
                    for doc in retrieved_docs:
                        source_name = doc.metadata.get('source', 'Bilinmiyor')
                        page_number = doc.metadata.get('page', None)
                        if page_number is not None:
                            page_number += 1
                        else:
                            page_number = 'BelirtilmemiÅŸ'
                        st.info(f"**Kaynak:** {source_name} - **Sayfa:** {page_number}")
                        st.caption(doc.page_content[:500] + "...")
                else:
                    st.warning("Kaynak bilgisi alÄ±namadÄ±.")

    st.session_state.messages.append({"role": "assistant", "content": full_response})

st.markdown("---")
st.caption("Bu asistanÄ±n bilgi tabanÄ±, Borsa Ä°stanbul SÃ¼rdÃ¼rÃ¼lebilirlik Rehberi ve Erdem & Erdem Ã‡SY Terimler SÃ¶zlÃ¼ÄŸÃ¼ dokÃ¼manlarÄ±ndan oluÅŸturulmuÅŸtur.")
