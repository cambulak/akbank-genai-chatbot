# app.py

# Gerekli kÃ¼tÃ¼phaneleri import ediyoruz
import os
import streamlit as st
import glob

# LangChain'in RAG mimarisi iÃ§in temel bileÅŸenleri
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.retrievers.multi_query import MultiQueryRetriever

# PDF okuma ve metin bÃ¶lme iÅŸlemleri iÃ§in kÃ¼tÃ¼phaneler
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# VektÃ¶r veritabanÄ±nÄ±n diskte saklanacaÄŸÄ± klasÃ¶r adÄ±
FAISS_INDEX_PATH = "faiss_index"


# --- RAG SÄ°STEMÄ°NÄ°N KURULUMU VE MODELLERÄ°N YÃœKLENMESÄ° ---

# @st.cache_resource, Streamlit'in bu fonksiyonu sadece bir kere Ã§alÄ±ÅŸtÄ±rmasÄ±nÄ± saÄŸlar.
# Bu, modellerin ve veritabanÄ±nÄ±n her kullanÄ±cÄ± etkileÅŸiminde yeniden yÃ¼klenmesini engelleyerek
# uygulamayÄ± inanÄ±lmaz derecede hÄ±zlandÄ±ran bir optimizasyon yÃ¶ntemidir.
@st.cache_resource
def load_and_build_db():
    """
    Uygulama baÅŸladÄ±ÄŸÄ±nda modelleri ve vektÃ¶r veritabanÄ±nÄ± yÃ¼kler veya yoksa oluÅŸturur.
    Bu fonksiyon, tÃ¼m aÄŸÄ±r yÃ¼kÃ¼ (model indirme, PDF iÅŸleme, veritabanÄ± oluÅŸturma)
    sadece uygulamanÄ±n ilk aÃ§Ä±lÄ±ÅŸÄ±nda bir kereliÄŸine yapar.
    """
    print("VeritabanÄ± kontrol ediliyor ve yÃ¼kleniyor...")

    # 1. EMBEDDING MODELÄ°
    # Metinleri anlamsal vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in kullanÄ±lacak model.
    # 'paraphrase-multilingual-mpnet-base-v2' modeli, TÃ¼rkÃ§e dahil birÃ§ok dili
    # yÃ¼ksek performansla anlama yeteneÄŸi sayesinde seÃ§ilmiÅŸtir. Bu, RAG sisteminin
    # "Retrieval" (Bilgi Getirme) adÄ±mÄ±nÄ±n kalitesini doÄŸrudan belirler.
    model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    embeddings = HuggingFaceEmbeddings(model_name=model_name)

    # 2. VEKTÃ–R VERÄ°TABANI
    # EÄŸer daha Ã¶nce oluÅŸturulmuÅŸ bir veritabanÄ± diskte varsa, onu yÃ¼kle.
    # Bu, uygulamanÄ±n yeniden baÅŸlatÄ±ldÄ±ÄŸÄ±nda PDF'leri tekrar iÅŸlemesini engeller.
    if os.path.exists(FAISS_INDEX_PATH):
        print("Mevcut veritabanÄ± bulundu, yÃ¼kleniyor.")
        db = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
    else:
        # EÄŸer veritabanÄ± yoksa (Ã¶rneÄŸin ilk Ã§alÄ±ÅŸtÄ±rma veya deploy sonrasÄ±)
        # 'data' klasÃ¶rÃ¼ndeki tÃ¼m PDF'lerden yeni bir veritabanÄ± oluÅŸtur.
        print("VeritabanÄ± bulunamadÄ±, PDF'lerden oluÅŸturuluyor...")

        # 'data' klasÃ¶rÃ¼ndeki tÃ¼m .pdf uzantÄ±lÄ± dosyalarÄ± bul.
        pdf_files = glob.glob("data/*.pdf")
        if not pdf_files:
            st.error("HATA: 'data' klasÃ¶rÃ¼nde okunacak PDF dosyasÄ± bulunamadÄ±.")
            st.stop()

        # TÃ¼m PDF'lerden gelen dokÃ¼manlarÄ± birleÅŸtirmek iÃ§in bir liste oluÅŸtur.
        all_documents = []
        for pdf_path in pdf_files:
            loader = PyPDFLoader(pdf_path)
            documents = loader.load()
            # Her bir dokÃ¼manÄ±n metadatasÄ±na, hangi dosyadan geldiÄŸi bilgisini ekliyoruz.
            # Bu, daha sonra cevap kaynaklarÄ±nÄ± gÃ¶sterirken Ã§ok iÅŸe yarayacak.
            for doc in documents:
                doc.metadata["source"] = os.path.basename(pdf_path)
            all_documents.extend(documents)

        # Metinleri daha kÃ¼Ã§Ã¼k ve yÃ¶netilebilir parÃ§alara (chunks) bÃ¶l.
        # chunk_overlap, parÃ§alar arasÄ± anlam bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ korumaya yardÄ±mcÄ± olur.
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
        docs = text_splitter.split_documents(all_documents)

        # ParÃ§alanmÄ±ÅŸ metinlerden ve embedding modelinden FAISS veritabanÄ±nÄ± oluÅŸtur.
        db = FAISS.from_documents(docs, embeddings)
        # OluÅŸturulan veritabanÄ±nÄ± diskte bir sonraki kullanÄ±m iÃ§in kaydet.
        db.save_local(FAISS_INDEX_PATH)
        print("VeritabanÄ± oluÅŸturuldu ve kaydedildi.")

    # 3. DÄ°L MODELÄ° (LLM)
    # CevaplarÄ± Ã¼retecek olan ana model. Google'Ä±n Gemini Pro modelini kullanÄ±yoruz.
    llm = ChatGoogleGenerativeAI(model="gemini-pro-latest", temperature=0.1, convert_system_message_to_human=True)

    # 4. RETRIEVER (BÄ°LGÄ° GETÄ°RÄ°CÄ°)
    # Bu, RAG sisteminin en Ã¶nemli optimizasyonlarÄ±ndan biridir.
    # KullanÄ±cÄ±nÄ±n tek bir sorusunu alÄ±p, LLM'i kullanarak o soruyu farklÄ± aÃ§Ä±lardan
    # yeniden ifade eden birden Ã§ok alt sorgu Ã¼retir. Bu, cevabÄ± dokÃ¼manlarÄ±n farklÄ±
    # yerlerine yayÄ±lmÄ±ÅŸ karmaÅŸÄ±k sorular iÃ§in bile ilgili tÃ¼m bilgi parÃ§alarÄ±nÄ± toplama
    # baÅŸarÄ±sÄ±nÄ± bÃ¼yÃ¼k Ã¶lÃ§Ã¼de artÄ±rÄ±r.
    base_retriever = db.as_retriever(search_kwargs={'k': 7})
    retriever = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=llm)

    print("Modeller ve Multi-Query Retriever baÅŸarÄ±yla hazÄ±rlandÄ±.")
    return retriever, llm


def create_rag_chain(retriever, llm):
    """
    Verilen retriever ve llm ile RAG (Retrieval-Augmented Generation) zincirini oluÅŸturur.
    Bu zincir, kullanÄ±cÄ± sorusundan nihai cevaba giden tÃ¼m adÄ±mlarÄ± dÃ¼zenler.
    """
    # 5. PROMPT ÅABLONU
    # LLM'e ne yapmasÄ± gerektiÄŸini sÃ¶yleyen talimatlar bÃ¼tÃ¼nÃ¼dÃ¼r.
    # {context} -> Retriever'dan gelen bilgi parÃ§alarÄ±
    # {question} -> KullanÄ±cÄ±nÄ±n orijinal sorusu
    # Bu ÅŸablon, modelin sadece kendisine verilen baÄŸlama sadÄ±k kalmasÄ±nÄ± saÄŸlayarak
    # "halÃ¼sinasyon" gÃ¶rmesini (bilgi uydurmasÄ±nÄ±) engeller.
    template = """
    ### TALÄ°MAT:
    Sana verilen `BAÄLAM` bÃ¶lÃ¼mÃ¼ndeki bilgileri kullanarak `SORU` bÃ¶lÃ¼mÃ¼ndeki soruyu yanÄ±tla. CevabÄ±n dÄ±ÅŸarÄ±dan bilgi iÃ§ermemelidir. CevabÄ±n net, anlaÅŸÄ±lÄ±r ve sohbet formatÄ±nda olsun. EÄŸer baÄŸlamda sorunun cevabÄ± yoksa, 'Bu konuda saÄŸlanan dokÃ¼manlarda bir bilgi bulamadÄ±m.' de. CevaplarÄ±nÄ± TÃ¼rkÃ§e ver.

    ### BAÄLAM:
    {context}

    ### SORU:
    {question}

    ### CEVAP:
    """
    prompt = PromptTemplate(template=template, input_variables=["context", "question"])

    # 6. LANGCHAIN EXPRESSION LANGUAGE (LCEL) ZÄ°NCÄ°RÄ°
    # Boru (pipe |) operatÃ¶rÃ¼ ile adÄ±mlarÄ± birbirine baÄŸlarÄ±z:
    # 1. `retriever` ve `question` paralel olarak Ã§alÄ±ÅŸÄ±r.
    # 2. Ã‡Ä±ktÄ±larÄ± `prompt` ÅŸablonuna beslenir.
    # 3. Doldurulan prompt `llm`'e gÃ¶nderilir.
    # 4. LLM'in cevabÄ± `StrOutputParser` ile temiz bir metne dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.
    return {"context": retriever, "question": RunnablePassthrough()} | prompt | llm | StrOutputParser()


# --- STREAMLIT ARAYÃœZÃœ ---

# Sayfa konfigÃ¼rasyonu (baÅŸlÄ±k, layout vb.)
st.set_page_config(page_title="Kurumsal SÃ¼rdÃ¼rÃ¼lebilirlik AsistanÄ±", layout="wide", initial_sidebar_state="expanded")

# Kenar Ã‡ubuÄŸu (Sidebar)
# ArayÃ¼zÃ¼ temiz tutmak iÃ§in bilgilendirici metinleri ve butonlarÄ± buraya koyuyoruz.
with st.sidebar:
    st.title("ğŸŒ± Kurumsal SÃ¼rdÃ¼rÃ¼lebilirlik AsistanÄ±")
    st.markdown("""
    Bu asistan, RAG mimarisi kullanarak aÅŸaÄŸÄ±daki belgelerdeki bilgilere gÃ¶re sorularÄ±nÄ±zÄ± yanÄ±tlar:
    - **Erdem & Erdem - Ã‡SY Terimler SÃ¶zlÃ¼ÄŸÃ¼**
    - **Borsa Ä°stanbul - SÃ¼rdÃ¼rÃ¼lebilirlik Rehberi**
    """)
    st.markdown("---")

    # Sohbet geÃ§miÅŸini temizlemek iÃ§in bir buton
    if st.button("Sohbeti Temizle", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.caption("Akbank GenAI Bootcamp Projesi")

# Ã–rnek Sorular BÃ¶lÃ¼mÃ¼
st.markdown("""
**Ã–rnek Sorular:**
- SÃ¼rdÃ¼rÃ¼lebilir uygulamalarÄ±n artÄ±rÄ±lmasÄ± ÅŸirkete hangi katkÄ±larÄ± saÄŸlar?
- SÄ±nÄ±rda karbon dÃ¼zenlemesi nedir?
- Paris AnlaÅŸmasÄ± nedir?
- Kurumsal YÃ¶netim nedir?
- Karbon tutma nedir?
""")
st.markdown("---")

# API AnahtarÄ±nÄ± Streamlit Secrets'tan (gÃ¼venli depolama alanÄ±) kontrol et
if 'GOOGLE_API_KEY' not in st.secrets:
    st.error("HATA: GOOGLE_API_KEY bulunamadÄ±. LÃ¼tfen Streamlit Cloud ayarlarÄ±ndan 'Secrets' bÃ¶lÃ¼mÃ¼ne ekleyin.")
    st.stop()

# Ana RAG sistemini baÅŸlat
try:
    retriever, llm = load_and_build_db()
    rag_chain = create_rag_chain(retriever, llm)
except Exception as e:
    st.error(f"BaÅŸlangÄ±Ã§ sÄ±rasÄ±nda bir hata oluÅŸtu: {e}")
    st.stop()

# Sohbet GeÃ§miÅŸi YÃ¶netimi
# st.session_state, Streamlit'in sayfayÄ± her yenilediÄŸinde deÄŸiÅŸkenleri hatÄ±rlamasÄ±nÄ± saÄŸlar.
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant",
         "content": "Merhaba! SÃ¼rdÃ¼rÃ¼lebilirlik veya Ã‡SY konularÄ±nda size nasÄ±l yardÄ±mcÄ± olabilirim?"}
    ]

# GeÃ§miÅŸ mesajlarÄ± ekrana yazdÄ±r
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# KullanÄ±cÄ±dan yeni girdi al ve sohbeti yÃ¼rÃ¼t
if prompt := st.chat_input("SÃ¼rdÃ¼rÃ¼lebilirlik stratejisi, raporlama veya bir terim hakkÄ±nda soru sorun..."):
    # KullanÄ±cÄ±nÄ±n sorusunu ekle
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AsistanÄ±n cevabÄ±nÄ± oluÅŸtur
    with st.chat_message("assistant"):
        # Cevap gelene kadar bir "bekleniyor" animasyonu gÃ¶ster
        with st.spinner("Ä°lgili belgeleri arÄ±yorum ve yanÄ±t oluÅŸturuyorum..."):
            # Ã–nce kaynaklarÄ± bul (cevapla birlikte gÃ¶stermek iÃ§in)
            retrieved_docs = retriever.get_relevant_documents(prompt)

            # CevabÄ± kelime kelime, akÄ±ÅŸ halinde (streaming) yazdÄ±r.
            # Bu, kullanÄ±cÄ±nÄ±n daha hÄ±zlÄ± geri bildirim almasÄ±nÄ± saÄŸlar.
            response_stream = rag_chain.stream(prompt)
            full_response = st.write_stream(response_stream)

            # Cevap yazdÄ±rÄ±ldÄ±ktan sonra, cevabÄ±n hangi kaynaklara dayandÄ±ÄŸÄ±nÄ± gÃ¶steren
            # geniÅŸletilebilir bir bÃ¶lÃ¼m ekle. Bu, chatbot'un gÃ¼venilirliÄŸini artÄ±rÄ±r.
            with st.expander("YanÄ±tÄ±n KaynaklarÄ±nÄ± GÃ¶r"):
                for doc in retrieved_docs:
                    source_name = doc.metadata.get('source', 'Bilinmiyor')
                    page_number = doc.metadata.get('page', 'Bilinmiyor')
                    if page_number != 'Bilinmiyor':
                        page_number += 1  # Sayfa numaralarÄ± 0'dan baÅŸladÄ±ÄŸÄ± iÃ§in 1 ekliyoruz
                    st.info(f"**Kaynak:** {source_name} - **Sayfa:** {page_number}")
                    st.caption(doc.page_content)

    # AsistanÄ±n tam cevabÄ±nÄ± sohbet geÃ§miÅŸine ekle
    st.session_state.messages.append({"role": "assistant", "content": full_response})

st.markdown("---")
st.caption(
    "Bu asistanÄ±n bilgi tabanÄ±, Borsa Ä°stanbul SÃ¼rdÃ¼rÃ¼lebilirlik Rehberi ve Erdem & Erdem Ã‡SY Terimler SÃ¶zlÃ¼ÄŸÃ¼ dokÃ¼manlarÄ±ndan oluÅŸturulmuÅŸtur.")
