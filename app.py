# app.py

import os
import streamlit as st  # Streamlit kÃ¼tÃ¼phanesini import ediyoruz
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser


# --- Konfigurasyon ve Model YÃ¼kleme FonksiyonlarÄ± ---
# Bu fonksiyonlar sadece bir kere Ã§alÄ±ÅŸacak ve sonuÃ§larÄ± cache'lenecek.
# Bu sayede her kullanÄ±cÄ± giriÅŸi olduÄŸunda modeller yeniden yÃ¼klenmeyecek.

@st.cache_resource
def load_models_and_db():
    """
    Uygulama baÅŸladÄ±ÄŸÄ±nda modelleri ve veritabanÄ±nÄ± yÃ¼kler.
    """
    print("Modeller ve veritabanÄ± yÃ¼kleniyor...")

    # Embedding Modelini YÃ¼kle
    model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    embeddings = HuggingFaceEmbeddings(model_name=model_name)

    # Yerel VektÃ¶r VeritabanÄ±nÄ± YÃ¼kle
    db = FAISS.load_local("faiss_index_csy_hf_new", embeddings, allow_dangerous_deserialization=True)

    # Retriever'Ä± OluÅŸtur
    retriever = db.as_retriever(search_kwargs={'k': 3})

    # Google Dil Modelini (LLM) TanÄ±mla
    KULLANILACAK_MODEL = "gemini-pro-latest"
    llm = ChatGoogleGenerativeAI(model=KULLANILACAK_MODEL, temperature=0.1, convert_system_message_to_human=True)

    print("Modeller ve veritabanÄ± baÅŸarÄ±yla yÃ¼klendi.")
    return retriever, llm


def create_rag_chain(retriever, llm):
    """
    Verilen retriever ve llm ile RAG zincirini oluÅŸturur.
    """
    template = """
    ### TALÄ°MAT:
    Sadece sana verilen `BAÄLAM` bÃ¶lÃ¼mÃ¼ndeki bilgileri kullanarak `SORU` bÃ¶lÃ¼mÃ¼ndeki soruyu yanÄ±tla. CevabÄ±n dÄ±ÅŸarÄ±dan bilgi iÃ§ermemelidir. EÄŸer baÄŸlamda sorunun cevabÄ± yoksa, 'Bu konuda saÄŸlanan dokÃ¼manda bir bilgi bulamadÄ±m.' de. CevaplarÄ±nÄ± TÃ¼rkÃ§e ve anlaÅŸÄ±lÄ±r bir dille yaz.

    ### BAÄLAM:
    {context}

    ### SORU:
    {question}

    ### CEVAP:
    """
    prompt = PromptTemplate(template=template, input_variables=["context", "question"])

    rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
    )
    return rag_chain


# --- Ana Streamlit UygulamasÄ± ---

# API anahtarÄ±nÄ± kontrol et (uygulama baÅŸlamadan Ã¶nce)
if "GOOGLE_API_KEY" not in os.environ:
    st.error("HATA: GOOGLE_API_KEY ortam deÄŸiÅŸkeni bulunamadÄ±. LÃ¼tfen ayarlayÄ±n.")
    st.stop()

# BaÅŸlÄ±k
st.title("ğŸ“– Ã‡SY Terimler SÃ¶zlÃ¼ÄŸÃ¼ Chatbot'u")
st.write(
    "Bu chatbot, Erdem & Erdem tarafÄ±ndan hazÄ±rlanan Ã‡SY Terimler SÃ¶zlÃ¼ÄŸÃ¼'ndeki bilgileri kullanarak sorularÄ±nÄ±zÄ± yanÄ±tlar.")

# Modelleri ve veritabanÄ±nÄ± yÃ¼kle
try:
    retriever, llm = load_models_and_db()
    rag_chain = create_rag_chain(retriever, llm)
except Exception as e:
    st.error(f"Modeller yÃ¼klenirken bir hata oluÅŸtu: {e}")
    st.stop()

# Sohbet geÃ§miÅŸini yÃ¶netmek iÃ§in session state kullan
if "messages" not in st.session_state:
    st.session_state.messages = []

# GeÃ§miÅŸ mesajlarÄ± ekrana yazdÄ±r
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# KullanÄ±cÄ±dan yeni bir girdi al
if prompt := st.chat_input("Ã‡SY ile ilgili bir terim sorun..."):
    # KullanÄ±cÄ±nÄ±n mesajÄ±nÄ± sohbet geÃ§miÅŸine ekle ve ekrana yazdÄ±r
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Chatbot'un cevabÄ±nÄ± oluÅŸtur
    with st.chat_message("assistant"):
        with st.spinner("DÃ¼ÅŸÃ¼nÃ¼yorum..."):
            response = rag_chain.invoke(prompt)
            st.markdown(response)

    # Chatbot'un cevabÄ±nÄ± sohbet geÃ§miÅŸine ekle
    st.session_state.messages.append({"role": "assistant", "content": response})